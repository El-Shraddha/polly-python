# This is a basic workflow to help you get started with Actions

name: Add Path

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  # This workflow contains a single job called "build"
  build:
    # The type of runner that the job will run on
    runs-on: ubuntu-20.04

    env:
      DEPLOY_REPO_NAME: ${{ github.repository }}
      BRANCH_NAME: ${{ github.head_ref || github.ref_name }}

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2
      
      - name: Run with setup-python 3.8
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
      - name: Install python dependencies
        run: pip install beautifulsoup4
        
      - name: Run the parsing in python
        run: |
          import os
          import json
          try: 
              from BeautifulSoup import BeautifulSoup
          except ImportError:
              from bs4 import BeautifulSoup
          from urllib.parse import urlparse, parse_qsl, urlencode
          
          directory = os.path.abspath(".")
          for filename in os.listdir(directory):
              f = os.path.join(directory, filename)
              if ( os.path.isfile(f) and os.path.splitext(f)[1] == ".ipynb" ):
                  f_p = open(f)
                  data = json.load(f_p)
                  for cell in data['cells']:
                      if 'metadata' in cell and 'id' in cell['metadata'] and cell['metadata']['id'] == "view-in-github":
                          print(f)
                          parsed_html = BeautifulSoup(cell['source'][0],"html.parser")
                          parts = urlparse(parsed_html.a['href'])
                          url_dict = dict(parse_qsl(parts.query))
                          if ( url_dict.get('source') == "github" ):
                              print(url_dict.get("path"))
                              print(os.environ["DEPLOY_REPO_NAME"])
                              print(os.environ["BRANCH_NAME"])
        shell: python
